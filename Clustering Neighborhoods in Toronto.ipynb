{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Clustering Neighbourhoods in Toronto\u00b6", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Converting Wikipedia html table into a DataFrame", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Pre-processing\u00b6", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import numpy as np # library to handle data in a vectorized manner\nimport pandas as pd # library for data analsysis\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nimport json # library to handle JSON files\n!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\nfrom geopy.geocoders import Nominatim # convert an address into latitude and longitude values\nimport requests # library to handle requests\nfrom pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n# Matplotlib and associated plotting modules\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\n#import k-means from clustering stage\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets.samples_generator import make_blobs\n!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\nimport folium # map rendering library\nfrom bs4 import BeautifulSoup\nimport lxml\nprint('Libraries imported.')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Solving environment: done\n\n## Package Plan ##\n\n  environment location: /opt/conda/envs/Python36\n\n  added / updated specs: \n    - geopy\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    openssl-1.1.1c             |       h516909a_0         2.1 MB  conda-forge\n    geopy-1.20.0               |             py_0          57 KB  conda-forge\n    geographiclib-1.49         |             py_0          32 KB  conda-forge\n    ca-certificates-2019.6.16  |       hecc5488_0         145 KB  conda-forge\n    certifi-2019.6.16          |           py36_1         149 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:         2.5 MB\n\nThe following NEW packages will be INSTALLED:\n\n    geographiclib:   1.49-py_0         conda-forge\n    geopy:           1.20.0-py_0       conda-forge\n\nThe following packages will be UPDATED:\n\n    ca-certificates: 2019.5.15-0                   --> 2019.6.16-hecc5488_0 conda-forge\n    certifi:         2019.6.16-py36_0              --> 2019.6.16-py36_1     conda-forge\n\nThe following packages will be DOWNGRADED:\n\n    openssl:         1.1.1c-h7b6447c_1             --> 1.1.1c-h516909a_0    conda-forge\n\n\nDownloading and Extracting Packages\nopenssl-1.1.1c       | 2.1 MB    | ##################################### | 100% \ngeopy-1.20.0         | 57 KB     | ##################################### | 100% \ngeographiclib-1.49   | 32 KB     | ##################################### | 100% \nca-certificates-2019 | 145 KB    | ##################################### | 100% \ncertifi-2019.6.16    | 149 KB    | ##################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /opt/conda/envs/Python36\n\n  added / updated specs: \n    - folium=0.5.0\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    branca-0.3.1               |             py_0          25 KB  conda-forge\n    folium-0.5.0               |             py_0          45 KB  conda-forge\n    altair-3.1.0               |           py36_0         724 KB  conda-forge\n    vincent-0.4.4              |             py_1          28 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:         822 KB\n\nThe following NEW packages will be INSTALLED:\n\n    altair:  3.1.0-py36_0 conda-forge\n    branca:  0.3.1-py_0   conda-forge\n    folium:  0.5.0-py_0   conda-forge\n    vincent: 0.4.4-py_1   conda-forge\n\n\nDownloading and Extracting Packages\nbranca-0.3.1         | 25 KB     | ##################################### | 100% \nfolium-0.5.0         | 45 KB     | ##################################### | 100% \naltair-3.1.0         | 724 KB    | ##################################### | 100% \nvincent-0.4.4        | 28 KB     | ##################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\nLibraries imported.\n"
                }
            ], 
            "execution_count": 2
        }, 
        {
            "source": "### Download, scrape and wrangle\u00b6\u00b6\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# download data and parse it:\nr = requests.get('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M')\nsoup = BeautifulSoup(r.text, 'html.parser')\ntable=soup.find('table', attrs={'class':'wikitable sortable'})\n\n#get headers:\nheaders=table.findAll('th')\nfor i, head in enumerate(headers): headers[i]=str(headers[i]).replace(\"<th>\",\"\").replace(\"</th>\",\"\").replace(\"\\n\",\"\")\n\n#Find all items and skip first one:\nrows=table.findAll('tr')\nrows=rows[1:len(rows)]\n\n# skip all meta symbols and line feeds between rows:\nfor i, row in enumerate(rows): rows[i] = str(rows[i]).replace(\"\\n</td></tr>\",\"\").replace(\"<tr>\\n<td>\",\"\")\n\n# make dataframe, expand rows and drop the old one:\ndf=pd.DataFrame(rows)\ndf[headers] = df[0].str.split(\"</td>\\n<td>\", n = 2, expand = True) \ndf.drop(columns=[0],inplace=True)\n\n# skip not assigned boroughs:\ndf = df.drop(df[(df.Borough == \"Not assigned\")].index)\n# give \"Not assigned\" Neighborhoods same name as Borough:\ndf.Neighbourhood.replace(\"Not assigned\", df.Borough, inplace=True)\n\n# copy Borough value to Neighborhood if NaN:\ndf.Neighbourhood.fillna(df.Borough, inplace=True)\n# drop duplicate rows:\ndf=df.drop_duplicates()\n\n# extract titles from columns\ndf.update(\n    df.Neighbourhood.loc[\n        lambda x: x.str.contains('title')\n    ].str.extract('title=\\\"([^\\\"]*)',expand=False))\n\ndf.update(\n    df.Borough.loc[\n        lambda x: x.str.contains('title')\n    ].str.extract('title=\\\"([^\\\"]*)',expand=False))\n\n# delete Toronto annotation from Neighbourhood:\ndf.update(\n    df.Neighbourhood.loc[\n        lambda x: x.str.contains('Toronto')\n    ].str.replace(\", Toronto\",\"\"))\ndf.update(\n    df.Neighbourhood.loc[\n        lambda x: x.str.contains('Toronto')\n    ].str.replace(\"\\(Toronto\\)\",\"\"))\n\n# combine multiple neighborhoods with the same post code\ndf2 = pd.DataFrame({'Postcode':df.Postcode.unique()})\ndf2['Borough']=pd.DataFrame(list(set(df['Borough'].loc[df['Postcode'] == x['Postcode']])) for i, x in df2.iterrows())\ndf2['Neighborhood']=pd.Series(list(set(df['Neighbourhood'].loc[df['Postcode'] == x['Postcode']])) for i, x in df2.iterrows())\ndf2['Neighborhood']=df2['Neighborhood'].apply(lambda x: ', '.join(x))\ndf2.dtypes\n\ndf2.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#add Geo-spatial data\ndfll= pd.read_csv(\"http://cocl.us/Geospatial_data\")\ndfll.rename(columns={'Postal Code':'Postcode'}, inplace=True)\ndfll.set_index(\"Postcode\")\ndf2.set_index(\"Postcode\")\ntoronto_data=pd.merge(df2, dfll)\ntoronto_data.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "toronto_data.shape\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "In order to define an instance of the geocoder, we need to define a user_agent. We will name our agent to_explorer, as shown below", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "address = 'Toronto, ON, Canada'\n\ngeolocator = Nominatim(user_agent=\"to_explorer\")\nlocation = geolocator.geocode(address)\nlatitude = location.latitude\nlongitude = location.longitude\nprint('The geograpical coordinate of Toronto, ON, Canada are {}, {}.'.format(latitude, longitude))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Create a map of Toronto with neighborhoods superimposed on top.\u00b6\u00b6\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# create map of New York using latitude and longitude values\nmap_toronto = folium.Map(location=[latitude, longitude], zoom_start=10)\n\n# add markers to map\nfor lat, lng, borough, neighborhood in zip(toronto_data['Latitude'], toronto_data['Longitude'], toronto_data['Borough'], toronto_data['Neighborhood']):\n    label = '{}, {}'.format(neighborhood, borough)\n    label = folium.Popup(label, parse_html=True)\n    folium.CircleMarker(\n        [lat, lng],\n        radius=5,\n        popup=label,\n        color='blue',\n        fill=True,\n        fill_color='#3186cc',\n        fill_opacity=0.7,\n        parse_html=False).add_to(map_toronto)  \n    \nmap_toronto", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Next, we are going to start utilizing the Foursquare API to explore the neighborhoods and segment them.\u00b6\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Define Foursquare Credentials and Version\u00b6\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "CLIENT_ID = 'S3ONPSOKENADL51USBUU4BATDOAQZUZ0WJL13VCXOPDXKQ33' # your Foursquare ID\nCLIENT_SECRET = '4HGT1KWEEAAPJYSWGNN0IGBOYHVVOMA01AAIH5Q5QSJXHH4T' # your Foursquare Secret\nVERSION = '20180605' # Foursquare API version\n\nprint('Your credentails:')\nprint('CLIENT_ID: ' + CLIENT_ID)\nprint('CLIENT_SECRET:' + CLIENT_SECRET)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Let's explore the first neighborhood in our dataframe.\u00b6\u00b6\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "toronto_data.loc[0, 'Neighborhood']\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "##### Get the neighborhood's latitude and longitude values.\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "neighborhood_latitude = toronto_data.loc[0, 'Latitude'] # neighborhood latitude value\nneighborhood_longitude = toronto_data.loc[0, 'Longitude'] # neighborhood longitude value\n\nneighborhood_name = toronto_data.loc[0, 'Neighborhood'] # neighborhood name\n\nprint('Latitude and longitude values of {} are {}, {}.'.format(neighborhood_name, \n                                                               neighborhood_latitude, \n                                                               neighborhood_longitude))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "LIMIT = 100 # limit of number of venues returned by Foursquare API\nradius = 500 # define radius\nurl = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n    CLIENT_ID, \n    CLIENT_SECRET, \n    VERSION, \n    neighborhood_latitude, \n    neighborhood_longitude, \n    radius, \n    LIMIT)\nurl # display URL\n\nresults = requests.get(url).json()\nresults", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "##### From the Foursquare lab in the previous module, we know that all the information is in the items key. Before we proceed, let's borrow the get_category_type function from the Foursquare lab", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# function that extracts the category of the venue\ndef get_category_type(row):\n    try:\n        categories_list = row['categories']\n    except:\n        categories_list = row['venue.categories']\n        \n    if len(categories_list) == 0:\n        return None\n    else:\n        return categories_list[0]['name']", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "##### Now we are ready to clean the json and structure it into a pandas dataframe.\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "venues = results['response']['groups'][0]['items']\n    \nnearby_venues = json_normalize(venues) # flatten JSON\n\n# filter columns\nfiltered_columns = ['venue.name', 'venue.categories', 'venue.location.lat', 'venue.location.lng']\nnearby_venues =nearby_venues.loc[:, filtered_columns]\n\n# filter the category for each row\nnearby_venues['venue.categories'] = nearby_venues.apply(get_category_type, axis=1)\n\n# clean columns\nnearby_venues.columns = [col.split(\".\")[-1] for col in nearby_venues.columns]\n\nnearby_venues.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "##### And how many venues were returned by Foursquare?\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "print('{} venues were returned by Foursquare.'.format(nearby_venues.shape[0]))\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### Explore Neighborhoods in Toronto\u00b6\u00b6\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "##### Let's create a function to repeat the same process to all the neighborhoods.\u00b6\u00b6\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def getNearbyVenues(names, latitudes, longitudes, radius=500):\n    \n    venues_list=[]\n    for name, lat, lng in zip(names, latitudes, longitudes):\n        print(name)\n            \n        # create the API request URL\n        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n            CLIENT_ID, \n            CLIENT_SECRET, \n            VERSION, \n            lat, \n            lng, \n            radius, \n            LIMIT)\n            \n        # make the GET request\n        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n        \n        # return only relevant information for each nearby venue\n        venues_list.append([(\n            name, \n            lat, \n            lng, \n            v['venue']['name'], \n            v['venue']['location']['lat'], \n            v['venue']['location']['lng'],  \n            v['venue']['categories'][0]['name']) for v in results])\n\n    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n    nearby_venues.columns = ['Neighborhood', \n                  'Neighborhood Latitude', \n                  'Neighborhood Longitude', \n                  'Venue', \n                  'Venue Latitude', \n                  'Venue Longitude', \n                  'Venue Category']\n    \n    return(nearby_venues)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "##### Now write the code to run the above function on each neighborhood and create a new dataframe called manhattan_venues.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "toronto_venues = getNearbyVenues(names=toronto_data['Neighborhood'],\n                                   latitudes=toronto_data['Latitude'],\n                                   longitudes=toronto_data['Longitude']\n                                  )", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "##### Let's check the size of the resulting dataframe\u00b6\u00b6\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "print(toronto_venues.shape)\ntoronto_venues.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "##### Let's check how many venues were returned for each neighborhood\u00b6\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "toronto_venues.groupby('Neighborhood').count()\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "##### Let's find out how many unique categories can be curated from all the returned venues\u00b6\u00b6\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "print('There are {} uniques categories.'.format(len(toronto_venues['Venue Category'].unique())))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### Analyze Each Neighborhood\u00b6\u00b6\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# one hot encoding\ntoronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n\n# add neighborhood column back to dataframe\ntoronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] \n\n# move neighborhood column to the first column\nfixed_columns = [toronto_onehot.columns[-1]] + list(toronto_onehot.columns[:-1])\ntoronto_onehot = toronto_onehot[fixed_columns]\n\ntoronto_onehot.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "##### And let's examine the new dataframe size.\u00b6\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "toronto_onehot.shape\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "##### Next, let's group rows by neighborhood and by taking the mean of the frequency of occurrence of each category\u00b6\u00b6\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "toronto_grouped = toronto_onehot.groupby('Neighborhood').mean().reset_index()\ntoronto_grouped", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "##### Let's confirm the new size\u00b6\u00b6\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "toronto_grouped.shape\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "##### Let's print each neighborhood along with the top 5 most common venues\u00b6\u00b6\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "num_top_venues = 5\n\nfor hood in toronto_grouped['Neighborhood']:\n    print(\"----\"+hood+\"----\")\n    temp = toronto_grouped[toronto_grouped['Neighborhood'] == hood].T.reset_index()\n    temp.columns = ['venue','freq']\n    temp = temp.iloc[1:]\n    temp['freq'] = temp['freq'].astype(float)\n    temp = temp.round({'freq': 2})\n    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))\n    print('\\n')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "##### Let's put that into a pandas dataframe\u00b6\u00b6\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def return_most_common_venues(row, num_top_venues):\n    row_categories = row.iloc[1:]\n    row_categories_sorted = row_categories.sort_values(ascending=False)\n    \n    return row_categories_sorted.index.values[0:num_top_venues]\n\nnum_top_venues = 10\n\nindicators = ['st', 'nd', 'rd']\n\n# create columns according to number of top venues\ncolumns = ['Neighborhood']\nfor ind in np.arange(num_top_venues):\n    try:\n        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n    except:\n        columns.append('{}th Most Common Venue'.format(ind+1))\n\n# create a new dataframe\nneighborhoods_venues_sorted = pd.DataFrame(columns=columns)\nneighborhoods_venues_sorted['Neighborhood'] = toronto_grouped['Neighborhood']\n\nfor ind in np.arange(toronto_grouped.shape[0]):\n    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(toronto_grouped.iloc[ind, :], num_top_venues)\n\nneighborhoods_venues_sorted.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### Cluster Neighborhoods\u00b6\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "##### Run k-means to cluster the neighborhood into 5 clusters.\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# set number of clusters\nkclusters = 5\n\ntoronto_grouped_clustering = toronto_grouped.drop('Neighborhood', 1)\n\n# run k-means clustering\nkmeans = KMeans(n_clusters=kclusters, random_state=0).fit(toronto_grouped_clustering)\n\n# check cluster labels generated for each row in the dataframe\nkmeans.labels_[0:10]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "##### Let's create a new dataframe that includes the cluster as well as the top 10 venues for each neighborhood.\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# add clustering labels\nneighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\n\ntoronto_merged = toronto_data\n\n# merge toronto_grouped with toronto_data to add latitude/longitude for each neighborhood\ntoronto_merged = toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')\n\n\n\n# check the last columns!\ntoronto_merged.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "##### Finally, let's visualize the resulting clusters\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# create map\nmap_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)\n\n# set color scheme for the clusters\nx = np.arange(kclusters)\nys = [i + x + (i*x)**2 for i in range(kclusters)]\ncolors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\nrainbow = [colors.rgb2hex(i) for i in colors_array]\n\n# add markers to the map\nmarkers_colors = []\nfor lat, lon, poi, cluster in zip(toronto_merged['Latitude'], toronto_merged['Longitude'], toronto_merged['Neighborhood'], toronto_merged['Cluster Labels']):\n    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n    folium.CircleMarker(\n        [lat, lon],\n        radius=5,\n        popup=label,\n        #color=rainbow[cluster-1],\n        fill=True,\n        #fill_color=rainbow[cluster-1],\n        fill_opacity=0.7).add_to(map_clusters)\n       \nmap_clusters", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### Examine Clusters\u00b6\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Cluster 1\u00b6\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "toronto_merged.loc[toronto_merged['Cluster Labels'] == 0, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Cluster 2\u00b6\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "toronto_merged.loc[toronto_merged['Cluster Labels'] == 1, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Cluster 3\u00b6\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Cluster 4\u00b6\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Cluster 5\u00b6\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.6.8", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}